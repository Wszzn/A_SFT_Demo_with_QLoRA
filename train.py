"""
è®­ç»ƒè„šæœ¬ - çŒ«å¨˜çš„è¯ç”Ÿ
"""

import os
import torch
import time
import logging
from transformers import (
    AutoTokenizer,
    AutoModelForCausalLM,
    TrainingArguments,
    Trainer,
    BitsAndBytesConfig,
    DataCollatorForSeq2Seq
)
from peft import get_peft_model, LoraConfig, TaskType

from config import Config, default_config
from dataset import  load_corpus2, CatGirlDataset
from generator import Generator

# è®¾ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


class PoemTrainer:
    """çŒ«å¨˜æ¨¡å‹è®­ç»ƒå™¨"""
    
    def __init__(self, config: Config = None):
        self.config = config or default_config
        self.model = None
        self.tokenizer = None
        self.trainer = None
        
        # è®¾ç½®éšæœºç§å­
        torch.manual_seed(self.config.random_seed)
        
    def setup_environment(self):
        """ç¯å¢ƒè®¾ç½®å’Œæ£€æŸ¥"""
        logger.info("ğŸ­ çŒ«å¨˜çš„è¯ç”Ÿ - Qwen3-8Bå¾®è°ƒå®ç°")
        logger.info("=" * 55)
        
        logger.info("ğŸ–¥ï¸  æ­£åœ¨æ£€æŸ¥è¿è¡Œç¯å¢ƒ...")
        if torch.cuda.is_available():
            gpu_name = torch.cuda.get_device_name(0)
            gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1e9
            logger.info(f"âœ… GPUå¯ç”¨: {gpu_name}")
            logger.info(f"   æ˜¾å­˜å®¹é‡: {gpu_memory:.1f} GB")
            logger.info(f"   CUDAç‰ˆæœ¬: {torch.version.cuda}")
        else:
            logger.warning("âš ï¸  GPUä¸å¯ç”¨ï¼Œå°†ä½¿ç”¨CPUè¿è¡Œï¼ˆé€Ÿåº¦ä¼šå¾ˆæ…¢ï¼‰")
        
        logger.info(f"ğŸ¯ ä½¿ç”¨è®¾å¤‡: {self.config.device}")
    
    def load_model_and_tokenizer(self):
        """åŠ è½½æ¨¡å‹å’Œåˆ†è¯å™¨"""
        logger.info("ğŸ¤– æ­£åœ¨åŠ è½½Qwenæ¨¡å‹...")
        logger.info(f"ğŸ“¦ æ¨¡å‹: {self.config.model.name}")
        
        # é‡åŒ–é…ç½®
        bnb_config = None
        if self.config.model.use_quantization:
            logger.info("âš™ï¸  é…ç½®4-bité‡åŒ–ä»¥èŠ‚çœæ˜¾å­˜...")
            bnb_config = BitsAndBytesConfig(
                load_in_4bit=self.config.model.load_in_4bit,
                bnb_4bit_use_double_quant=self.config.model.bnb_4bit_use_double_quant,
                bnb_4bit_quant_type=self.config.model.bnb_4bit_quant_type,
                bnb_4bit_compute_dtype=self.config.model.bnb_4bit_compute_dtype
            )
        
        # åŠ è½½åˆ†è¯å™¨
        logger.info("ğŸ”¤ åŠ è½½åˆ†è¯å™¨...")
        self.tokenizer = AutoTokenizer.from_pretrained(
            self.config.model.name, 
            trust_remote_code=self.config.model.trust_remote_code
        )
        self.tokenizer.pad_token = self.tokenizer.eos_token
        logger.info(f"âœ… åˆ†è¯å™¨è¯æ±‡è¡¨å¤§å°: {len(self.tokenizer):,}")
        
        # åŠ è½½æ¨¡å‹
        logger.info("ğŸ§  åŠ è½½é¢„è®­ç»ƒæ¨¡å‹...")
        start_time = time.time()
        self.model = AutoModelForCausalLM.from_pretrained(
            self.config.model.name,
            quantization_config=bnb_config,
            device_map=self.config.model.device_map,
            trust_remote_code=self.config.model.trust_remote_code
        )
        load_time = time.time() - start_time
        logger.info(f"âœ… æ¨¡å‹åŠ è½½å®Œæˆ (è€—æ—¶: {load_time:.2f}ç§’)")
    
    def setup_lora(self):
        """è®¾ç½®LoRAé…ç½®"""
        logger.info("ğŸ”§ é…ç½®LoRAå¾®è°ƒå‚æ•°...")
        lora_config = LoraConfig(
            task_type=TaskType.CAUSAL_LM,
            r=self.config.lora.r,
            lora_alpha=self.config.lora.lora_alpha,
            lora_dropout=self.config.lora.lora_dropout,
            target_modules=self.config.lora.target_modules,
            bias=self.config.lora.bias,
        )
        
        self.model = get_peft_model(self.model, lora_config)
        
        # ç»Ÿè®¡å‚æ•°
        trainable_params = 0
        total_params = 0
        for param in self.model.parameters():
            total_params += param.numel()
            if param.requires_grad:
                trainable_params += param.numel()
        
        logger.info("ğŸ“Š å¯è®­ç»ƒå‚æ•°ç»Ÿè®¡:")
        logger.info(f"   ğŸ“ˆ å¯è®­ç»ƒå‚æ•°: {trainable_params:,}")
        logger.info(f"   ğŸ“Š æ€»å‚æ•°é‡: {total_params:,}")
        logger.info(f"   ğŸ¯ è®­ç»ƒå‚æ•°å æ¯”: {100 * trainable_params / total_params:.3f}%")
    
    def prepare_dataset(self):
        """å‡†å¤‡è®­ç»ƒæ•°æ®"""
        logger.info("ğŸ“š æ­£åœ¨åŠ è½½çŒ«å¨˜è¯­å½•...")
        
        # åŠ è½½è¯­æ–™
        corpus = load_corpus2(self.config.data.corpus_file)
        
        # é™åˆ¶æ•°æ®å¤§å°
        max_size = self.config.training.max_data_size
        actual_size = min(len(corpus), max_size)
        logger.info(f"ğŸ“Š ä½¿ç”¨æ•°æ®é‡: {actual_size:,} / {len(corpus):,}")
        
        # åˆ›å»ºæ•°æ®é›†
        logger.info("ğŸ¯ å‡†å¤‡è®­ç»ƒæ•°æ®...")
        self.dataset = CatGirlDataset(
            corpus[:actual_size], 
            self.tokenizer,
            max_len=self.config.training.max_sequence_length
        )
        
        if len(self.dataset) == 0:
            raise ValueError("æ²¡æœ‰æœ‰æ•ˆçš„è®­ç»ƒæ•°æ®")
        
        logger.info(f"âœ… è®­ç»ƒé›†å‡†å¤‡å®Œæˆ: {len(self.dataset):,} ä¸ªçŒ«å¨˜å¯¹è¯æ ·æœ¬")
        
        # æ˜¾ç¤ºæ ·æœ¬
        logger.info("ğŸ“ è®­ç»ƒæ ·æœ¬é¢„è§ˆ:")
        for i, sample in enumerate(self.dataset.get_sample_dialogues(3)):
            logger.info(f"   {i+1}. æˆ‘: {sample['instruction']}")
            logger.info(f"      çŒ«å¨˜: {sample['output']}")
    
    def setup_trainer(self):
        """è®¾ç½®è®­ç»ƒå™¨"""
        logger.info("âš™ï¸  è®­ç»ƒé…ç½®:")
        
        # è®­ç»ƒå‚æ•°
        training_args = TrainingArguments(
            output_dir=self.config.training.output_dir,
            per_device_train_batch_size=self.config.training.per_device_train_batch_size,
            gradient_accumulation_steps=self.config.training.gradient_accumulation_steps,
            num_train_epochs=self.config.training.num_train_epochs,
            learning_rate=self.config.training.learning_rate,
            lr_scheduler_type=self.config.training.lr_scheduler_type,
            warmup_ratio=self.config.training.warmup_ratio,
            logging_dir=self.config.training.logging_dir,
            logging_steps=self.config.training.logging_steps,
            save_strategy=self.config.training.save_strategy,
            bf16=self.config.training.bf16,
            report_to=self.config.training.report_to,
        )
        
        logger.info(f"   ğŸ“Š æ‰¹æ¬¡å¤§å°: {training_args.per_device_train_batch_size}")
        logger.info(f"   ğŸ”„ æ¢¯åº¦ç´¯ç§¯æ­¥æ•°: {training_args.gradient_accumulation_steps}")
        logger.info(f"   ğŸ¯ è®­ç»ƒè½®æ•°: {training_args.num_train_epochs}")
        logger.info(f"   ğŸ“ˆ å­¦ä¹ ç‡: {training_args.learning_rate}")
        
        # æ•°æ®æ•´ç†å™¨
        data_collator = DataCollatorForSeq2Seq(tokenizer=self.tokenizer, padding=True)
        
        # åˆå§‹åŒ–Trainer
        self.trainer = Trainer(
            model=self.model,
            args=training_args,
            train_dataset=self.dataset,
            data_collator=data_collator,
        )
        
        # è®¡ç®—è®­ç»ƒæ­¥æ•°
        total_steps = len(self.dataset) // (
            training_args.per_device_train_batch_size * 
            training_args.gradient_accumulation_steps
        )
        logger.info(f"ğŸ“Š é¢„è®¡è®­ç»ƒæ­¥æ•°: {total_steps}")
    
    def train(self):
        """å¼€å§‹è®­ç»ƒ"""
        logger.info("ğŸš€ å¼€å§‹å¾®è°ƒè®­ç»ƒ...")
        logger.info("=" * 50)
        
        training_start = time.time()
        
        try:
            self.trainer.train()
            training_time = time.time() - training_start
            logger.info(f"ğŸ‰ è®­ç»ƒå®Œæˆ! æ€»è€—æ—¶: {training_time:.2f}ç§’ ({training_time/60:.1f}åˆ†é’Ÿ)")
            return training_time
        except Exception as e:
            logger.error(f"âŒ è®­ç»ƒè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {str(e)}")
            raise
    
    def save_model(self, save_path: str = None):
        """ä¿å­˜æ¨¡å‹"""
        if save_path is None:
            save_path = "./qwen-cat-final"
        
        logger.info(f"ğŸ’¾ ä¿å­˜å¾®è°ƒæ¨¡å‹åˆ°: {save_path}")
        self.trainer.save_model(save_path)
        logger.info("âœ… æ¨¡å‹ä¿å­˜å®Œæˆ")
        return save_path
    
    def evaluate_model(self, save_path: str):
        """è¯„ä¼°æ¨¡å‹"""
        logger.info("ğŸ­ æµ‹è¯•å¾®è°ƒåçš„æ¨¡å‹")
        logger.info("=" * 40)
        
        # åŠ è½½ç”Ÿæˆå™¨
        generator = Generator(save_path, self.config.model.name)
        
        # æµ‹è¯•æ ·æœ¬
        test_lines = [
            "ä½ æ˜¯è°ï¼Ÿ",
        ]
        
        logger.info("ğŸ¨ AIçŒ«å¨˜å±•ç¤º:")
        logger.info("-" * 30)
        
        results = generator.evaluate_samples(test_lines)
        
        for i, result in enumerate(results["results"], 1):
            logger.info(f"ğŸ“ æµ‹è¯• {i}/{len(test_lines)}:")
            logger.info(f"âœ¨ æˆ‘: {result['input']}")
            logger.info(f"ğŸ¯ çŒ«å¨˜: {result['output']}")
            logger.info(f"â±ï¸  ç”Ÿæˆç”¨æ—¶: {result['time']:.2f}ç§’")
            logger.info("â”€" * 25)
        
        return results
    
    def run_full_training(self):
        """å®Œæ•´è®­ç»ƒæµç¨‹"""
        try:
            # 1. ç¯å¢ƒè®¾ç½®
            self.setup_environment()
            
            # 2. åŠ è½½æ¨¡å‹
            self.load_model_and_tokenizer()
            
            # 3. è®¾ç½®LoRA
            self.setup_lora()
            
            # 4. å‡†å¤‡æ•°æ®
            self.prepare_dataset()
            
            # 5. è®¾ç½®è®­ç»ƒå™¨
            self.setup_trainer()
            
            # 6. è®­ç»ƒ
            training_time = self.train()
            
            # 7. ä¿å­˜æ¨¡å‹
            save_path = self.save_model()
            
            # 8. è¯„ä¼°æ¨¡å‹
            eval_results = self.evaluate_model(save_path)
            
            # 9. è®­ç»ƒæ€»ç»“
            logger.info("ğŸŠ ç¨‹åºæ‰§è¡Œå®Œæˆ!")
            logger.info("ğŸ“Š è®­ç»ƒæ€»ç»“:")
            logger.info(f"   ğŸ¯ è®­ç»ƒæ•°æ®: {len(self.dataset):,} ä¸ªå¯¹è”")
            logger.info(f"   â±ï¸  è®­ç»ƒæ—¶é•¿: {training_time:.2f}ç§’")
            logger.info(f"   ğŸ’¾ æ¨¡å‹ä¿å­˜: {save_path}")
            logger.info(f"   ğŸ­ æµ‹è¯•æ ·æœ¬: {len(eval_results['results'])} ä¸ª")
            logger.info("âœ¨ çŒ«å¨˜è¯ç”Ÿ! âœ¨")
            
            return {
                "model_path": save_path,
                "training_time": training_time,
                "dataset_size": len(self.dataset),
                "eval_results": eval_results
            }
            
        except Exception as e:
            logger.error(f"âŒ è®­ç»ƒå¤±è´¥: {str(e)}")
            raise


def main():
    """ä¸»å‡½æ•°"""
    trainer = PoemTrainer()
    results = trainer.run_full_training()
    return results


if __name__ == "__main__":
    main()