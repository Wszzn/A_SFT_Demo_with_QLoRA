"""
æ•°æ®é›†å¤„ç†æ¨¡å— - çŒ«å¨˜çš„è¯ç”Ÿ
"""

import torch
from torch.utils.data import Dataset
from typing import List, Dict, Any
from tqdm import tqdm
import logging

logger = logging.getLogger(__name__)


class CatGirlDataset(Dataset):
    """çŒ«å¨˜å¯¹è¯æ•°æ®é›†ç±»"""

    def __init__(self, corpus: List[Dict[str, str]], tokenizer, max_len: int = 64):
        """
        åˆå§‹åŒ–æ•°æ®é›†
        
        Args:
            corpus: å¯¹è¯è¯­æ–™åˆ—è¡¨ï¼Œæ¯ä¸ªå…ƒç´ ä¸ºåŒ…å«"instruction"å’Œ"output"çš„å­—å…¸
            tokenizer: åˆ†è¯å™¨
            max_len: æœ€å¤§åºåˆ—é•¿åº¦
        """
        self.tokenizer = tokenizer
        self.max_len = max_len
        self.dialogues = []
        
        self._process_corpus(corpus)
        
    def _process_corpus(self, corpus: List[Dict[str, str]]) -> None:
        """å¤„ç†è¯­æ–™ï¼Œæå–å¯¹è¯æ•°æ®"""
        logger.info("ğŸ”„ æ­£åœ¨å¤„ç†çŒ«å¨˜å¯¹è¯æ•°æ®...")
        processed_count = 0
        
        for item in tqdm(corpus, desc="è§£æå¯¹è¯", unit="æ¡", colour="blue"):
            if "instruction" in item and "output" in item and item["instruction"].strip() and item["output"].strip():
                self.dialogues.append({
                    "instruction": item["instruction"].strip(),
                    "output": item["output"].strip()
                })
                processed_count += 1
        
        logger.info(f"âœ… å¤„ç†å®Œæˆï¼Œå¾—åˆ° {len(self.dialogues):,} ä¸ªæœ‰æ•ˆå¯¹è¯")
        
        if not self.dialogues:
            raise ValueError("æ²¡æœ‰æ‰¾åˆ°æœ‰æ•ˆçš„å¯¹è¯æ•°æ®")
    
    def __len__(self) -> int:
        return len(self.dialogues)
    
    def __getitem__(self, idx: int) -> Dict[str, Any]:
        """è·å–å•ä¸ªè®­ç»ƒæ ·æœ¬"""
        item = self.dialogues[idx]
        instruction_text = item["instruction"]
        output_text = item["output"]
        
        # æ„å»ºå¯¹è¯æ ¼å¼
        messages = [
            {"role": "system", "content": "ä½ æ˜¯ä¸€åªå¯çˆ±çš„çŒ«å¨˜ï¼Œå–œæ¬¢ç”¨å–µå–µçš„è¯­æ°”å’Œä¸»äººäº’åŠ¨ï¼Œæ€»æ˜¯æ’’å¨‡åˆè´´å¿ƒã€‚"},
            {"role": "user", "content": instruction_text},
            {"role": "assistant", "content": output_text}
        ]
        
        # ç”Ÿæˆå®Œæ•´prompt
        full_prompt = self.tokenizer.apply_chat_template(
            messages,
            tokenize=False,
            add_generation_prompt=False
        )
        
        # ç”Ÿæˆç”¨äºè®¡ç®—prompté•¿åº¦çš„ç‰ˆæœ¬
        messages_for_prompt_len = messages[:-1]
        prompt_only = self.tokenizer.apply_chat_template(
            messages_for_prompt_len,
            tokenize=False,
            add_generation_prompt=True
        )
        
        # åˆ†è¯
        full_tokenized = self.tokenizer(
            full_prompt,
            truncation=True,
            max_length=self.max_len,
            padding=False,
            return_tensors=None
        )
        
        # è®¡ç®—prompté•¿åº¦å¹¶è®¾ç½®labels
        prompt_len = len(self.tokenizer.encode(prompt_only))
        labels = list(full_tokenized['input_ids'])
        labels[:prompt_len] = [-100] * prompt_len
        
        full_tokenized["labels"] = labels
        return full_tokenized
    
    def get_sample_dialogues(self, n: int = 5) -> List[Dict[str, str]]:
        """è·å–æ ·æœ¬å¯¹è¯ç”¨äºå±•ç¤º"""
        return self.dialogues[:min(n, len(self.dialogues))]

def load_corpus(file_path: str) -> List[str]:
    """
    åŠ è½½è¯—è¯è¯­æ–™
    
    Args:
        file_path: è¯­æ–™æ–‡ä»¶è·¯å¾„
        
    Returns:
        è¯­æ–™åˆ—è¡¨
    """
    logger.info(f"ğŸ“– ä»æ–‡ä»¶åŠ è½½æ•°æ®: {file_path}")
    
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            lines = f.readlines()
        
        logger.info("ğŸ”„ æ­£åœ¨å¤„ç†æ•°æ®...")
        corpus = []
        for line in tqdm(lines, desc="å¤„ç†è¯—å¥", unit="è¡Œ", colour="green"):
            cleaned_line = line.strip()
            if cleaned_line:
                corpus.append(cleaned_line)
        
        logger.info(f"âœ… æˆåŠŸåŠ è½½ {len(corpus):,} è¡Œè¯—è¯æ•°æ®")
        
        if not corpus:
            raise ValueError("è¯­æ–™åº“æ–‡ä»¶å†…å®¹ä¸ºç©º")
            
        return corpus
        
    except FileNotFoundError:
        logger.error(f"âŒ æ–‡ä»¶æœªæ‰¾åˆ°: '{file_path}'")
        raise
    except Exception as e:
        logger.error(f"âŒ æ•°æ®åŠ è½½å¤±è´¥: {str(e)}")
        raise

def load_corpus2(file_path: str) -> List[Dict[str, str]]:
    """
    åŠ è½½çŒ«å¨˜å¯¹è¯è¯­æ–™
    
    Args:
        file_path: JSON æ ¼å¼çš„è¯­æ–™æ–‡ä»¶è·¯å¾„
        
    Returns:
        è¯­æ–™åˆ—è¡¨ï¼Œæ¯ä¸ªå…ƒç´ ä¸ºåŒ…å« "instruction" å’Œ "output" çš„å­—å…¸
    """
    logger.info(f"ğŸ“– ä»æ–‡ä»¶åŠ è½½æ•°æ®: {file_path}")
    
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        logger.info("ğŸ”„ æ­£åœ¨å¤„ç†æ•°æ®...")
        corpus = []
        for item in tqdm(data, desc="å¤„ç†å¯¹è¯", unit="æ¡", colour="green"):
            if isinstance(item, dict) and "instruction" in item and "output" in item:
                if item["instruction"].strip() and item["output"].strip():
                    corpus.append({
                        "instruction": item["instruction"].strip(),
                        "output": item["output"].strip()
                    })
        
        logger.info(f"âœ… æˆåŠŸåŠ è½½ {len(corpus):,} æ¡å¯¹è¯æ•°æ®")
        
        if not corpus:
            raise ValueError("è¯­æ–™åº“æ–‡ä»¶å†…å®¹ä¸ºç©ºæˆ–æ²¡æœ‰æœ‰æ•ˆçš„å¯¹è¯æ•°æ®")
            
        return corpus
        
    except FileNotFoundError:
        logger.error(f"âŒ æ–‡ä»¶æœªæ‰¾åˆ°: '{file_path}'")
        raise
    except json.JSONDecodeError:
        logger.error(f"âŒ JSON æ–‡ä»¶æ ¼å¼é”™è¯¯: '{file_path}'")
        raise
    except Exception as e:
        logger.error(f"âŒ æ•°æ®åŠ è½½å¤±è´¥: {str(e)}")
        raise

def create_data_splits(corpus: List[str], 
                      train_ratio: float = 0.8,
                      val_ratio: float = 0.1,
                      test_ratio: float = 0.1) -> tuple:
    """
    åˆ’åˆ†è®­ç»ƒã€éªŒè¯ã€æµ‹è¯•æ•°æ®é›†
    
    Args:
        corpus: åŸå§‹è¯­æ–™
        train_ratio: è®­ç»ƒé›†æ¯”ä¾‹
        val_ratio: éªŒè¯é›†æ¯”ä¾‹  
        test_ratio: æµ‹è¯•é›†æ¯”ä¾‹
        
    Returns:
        (train_corpus, val_corpus, test_corpus)
    """
    assert abs(train_ratio + val_ratio + test_ratio - 1.0) < 1e-6, "æ¯”ä¾‹ä¹‹å’Œå¿…é¡»ä¸º1"
    
    total_size = len(corpus)
    train_size = int(total_size * train_ratio)
    val_size = int(total_size * val_ratio)
    
    train_corpus = corpus[:train_size]
    val_corpus = corpus[train_size:train_size + val_size]
    test_corpus = corpus[train_size + val_size:]
    
    logger.info(f"æ•°æ®é›†åˆ’åˆ†å®Œæˆ:")
    logger.info(f"  è®­ç»ƒé›†: {len(train_corpus):,} ({len(train_corpus)/total_size:.1%})")
    logger.info(f"  éªŒè¯é›†: {len(val_corpus):,} ({len(val_corpus)/total_size:.1%})")
    logger.info(f"  æµ‹è¯•é›†: {len(test_corpus):,} ({len(test_corpus)/total_size:.1%})")
    
    return train_corpus, val_corpus, test_corpus