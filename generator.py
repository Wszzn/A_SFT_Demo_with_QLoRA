"""
ç”Ÿæˆå™¨  -  çŒ«å¨˜çš„è¯ç”Ÿ
"""

import torch
from transformers import AutoTokenizer, AutoModelForCausalLM
from peft import PeftModel
from typing import List, Dict, Any, Optional
import logging
from config import GenerationConfig

logger = logging.getLogger(__name__)


class Generator:
    """ç”Ÿæˆå™¨ç±»"""
    
    def __init__(self, 
                 model_path: str,
                 base_model_name: str = "../../Qwen3-8B",
                 device: Optional[str] = None):
        """
        åˆå§‹åŒ–ç”Ÿæˆå™¨
        
        Args:
            model_path: å¾®è°ƒæ¨¡å‹è·¯å¾„
            base_model_name: åŸºç¡€æ¨¡å‹åç§°
            device: è®¾å¤‡ç±»å‹
        """
        self.model_path = model_path
        self.base_model_name = base_model_name
        self.device = device or ("cuda" if torch.cuda.is_available() else "cpu")
        
        self._load_model()
        
    def _load_model(self):
        """åŠ è½½æ¨¡å‹å’Œåˆ†è¯å™¨"""
        logger.info("ğŸ¤– æ­£åœ¨åŠ è½½æ¨¡å‹...")
        
        # åŠ è½½åˆ†è¯å™¨
        self.tokenizer = AutoTokenizer.from_pretrained(
            self.base_model_name, 
            trust_remote_code=True
        )
        self.tokenizer.pad_token = self.tokenizer.eos_token
        
        # åŠ è½½åŸºç¡€æ¨¡å‹
        self.model = AutoModelForCausalLM.from_pretrained(
            self.base_model_name,
            torch_dtype=torch.float16,
            device_map="auto",
            trust_remote_code=True
        )
        
        # åŠ è½½LoRAé€‚é…å™¨
        try:
            self.model = PeftModel.from_pretrained(self.model, self.model_path)
            logger.info("âœ… LoRAé€‚é…å™¨åŠ è½½æˆåŠŸ")
        except Exception as e:
            logger.warning(f"âš ï¸ LoRAé€‚é…å™¨åŠ è½½å¤±è´¥ï¼Œä½¿ç”¨åŸºç¡€æ¨¡å‹: {e}")
        
        self.model.eval()
        logger.info(f"âœ… æ¨¡å‹åŠ è½½å®Œæˆï¼Œè®¾å¤‡: {self.device}")
    
    def generate_couplet(self, 
                        first_line: str,
                        config: Optional[GenerationConfig] = None) -> str:
        """
        ç”Ÿæˆå¯¹è¯
        
        Args:
            first_line: {ä½ çš„å¯¹è¯}
            config: ç”Ÿæˆé…ç½®
            
        Returns:
            {çŒ«å¨˜çš„å›ç­”}
        """
        if config is None:
            config = GenerationConfig()
            
        # æ„å»ºprompt
        messages = [
            {"role": "system", "content": "ä½ æ˜¯ä¸€åªå¯çˆ±çš„çŒ«å¨˜ï¼Œå–œæ¬¢ç”¨å–µå–µçš„è¯­æ°”å’Œä¸»äººäº’åŠ¨ï¼Œæ€»æ˜¯æ’’å¨‡åˆè´´å¿ƒã€‚"},
            {"role": "user", "content": f"{first_line}"}
        ]

        
        prompt = self.tokenizer.apply_chat_template(
            messages,
            tokenize=False,
            add_generation_prompt=True
        )
        
        # åˆ†è¯
        inputs = self.tokenizer(prompt, return_tensors="pt").to(self.device)
        
        # ç”Ÿæˆ
        with torch.no_grad():
            outputs = self.model.generate(
                **inputs,
                max_new_tokens=config.max_new_tokens,
                eos_token_id=self.tokenizer.eos_token_id,
                pad_token_id=self.tokenizer.pad_token_id,
                do_sample=config.do_sample,
                temperature=config.temperature,
                top_p=config.top_p,
                repetition_penalty=config.repetition_penalty,
            )
        
        # è§£ç å“åº”
        response = self.tokenizer.decode(
            outputs[0][inputs.input_ids.shape[1]:], 
            skip_special_tokens=True
        )
        
        # æå–å›ç­”
        if "</think>" in response:
            return response.split("</think>")[1].strip()
        return response.strip()
    
    def batch_generate(self, 
                      first_lines: List[str],
                      config: Optional[GenerationConfig] = None) -> List[str]:
        """
        ç”Ÿæˆå™¨æ‰¹é‡ç”Ÿæˆ
        
        Args:
            first_lines: ä¸Šè”åˆ—è¡¨
            config: ç”Ÿæˆé…ç½®
            
        Returns:
            ä¸‹è”åˆ—è¡¨
        """
        results = []
        for line in first_lines:
            result = self.generate_couplet(line, config)
            results.append(result)
        return results
    
    def interactive_mode(self):
        """äº¤äº’å¼ç”Ÿæˆæ¨¡å¼"""
        print("ğŸ­ çŒ«å¨˜çš„è¯ç”Ÿ - äº¤äº’æ¨¡å¼")
        print("è¾“å…¥æ‚¨çš„æ¶ˆæ¯ï¼ŒçŒ«å¨˜å°†å›å¤æ‚¨")
        print("è¾“å…¥ 'quit' æˆ– 'exit' é€€å‡º")
        print("=" * 50)
        
        while True:
            try:
                user_input = input("\nè¯·è¾“å…¥æ¶ˆæ¯: ").strip()
                
                if user_input.lower() in ['quit', 'exit', 'é€€å‡º']:
                    print("ğŸ‘‹ å†è§ï¼")
                    break
                
                if not user_input:
                    print("âš ï¸ è¯·è¾“å…¥æœ‰æ•ˆçš„æ¶ˆæ¯")
                    continue
                
                print("ğŸ¤” AIæ­£åœ¨æ€è€ƒ...")
                result = self.generate_couplet(user_input)
                
                print(f"âœ¨ ä½ çš„æ¶ˆæ¯: {user_input}")
                print(f"ğŸ¯ çŒ«å¨˜å›å¤: {result}")
                print("-" * 30)
                
            except KeyboardInterrupt:
                print("\nğŸ‘‹ ç¨‹åºå·²é€€å‡º")
                break
            except Exception as e:
                print(f"âŒ ç”Ÿæˆå¤±è´¥: {e}")
    
    def evaluate_samples(self, test_samples: List[str]) -> Dict[str, Any]:
        """
        è¯„ä¼°æ ·æœ¬ç”Ÿæˆè´¨é‡
        
        Args:
            test_samples: æµ‹è¯•æ ·æœ¬åˆ—è¡¨
            
        Returns:
            è¯„ä¼°ç»“æœ
        """
        results = []
        total_time = 0
        
        for sample in test_samples:
            import time
            start_time = time.time()
            
            generated = self.generate_couplet(sample)
            
            gen_time = time.time() - start_time
            total_time += gen_time
            
            results.append({
                "input": sample,
                "output": generated,
                "time": gen_time
            })
        
        return {
            "results": results,
            "total_time": total_time,
            "avg_time": total_time / len(test_samples),
            "samples_count": len(test_samples)
        }


def load_generator(model_path: str, base_model_name: str = "Qwen/Qwen3-0.6B") -> Generator:
    """
    å¿«é€ŸåŠ è½½ç”Ÿæˆå™¨
    
    Args:
        model_path: æ¨¡å‹è·¯å¾„
        base_model_name: åŸºç¡€æ¨¡å‹åç§°
        
    Returns:
        PoemGeneratorå®ä¾‹
    """
    return Generator(model_path, base_model_name)